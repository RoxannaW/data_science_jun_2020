{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595574066839",
   "display_name": "Python 3.6.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From HTML\n",
    "\n",
    "*Using only beautiful soap*\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Each row of the dataframe must have in different columns:\n",
    "\n",
    "- The name of the title\n",
    "- The id of the div where is the value scraped. If there is not id, then the value is must be numpy.nan\n",
    "- The name of the tag where is the value scraped.\n",
    "- The next scraped values in different rows: \n",
    "    - The value: \"Este es el segundo párrafo\"  --> Row 1\n",
    "    - The url https://pagina1.xyz/ --> Row 2\n",
    "    - The url https://pagina4.xyz/ --> Row 3\n",
    "    - The url https://pagina5.xyz/ --> Row 4\n",
    "    - The value \"links footer-links\" --> Row 5\n",
    "    - The value \"Este párrafo está en el footer\" --> Row 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "id_div = []\n",
    "tag_name = []\n",
    "value = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = \"\"\"<html lang=\"es\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Página de prueba</title>\n",
    "</head>\n",
    "<body>\n",
    "<div id=\"main\" class=\"full-width\">\n",
    "    <h1>El título de la página</h1>\n",
    "    <p>Este es el primer párrafo</p>\n",
    "    <p>Este es el segundo párrafo</p>\n",
    "    <div id=\"innerDiv\">\n",
    "        <div class=\"links\">\n",
    "            <a href=\"https://pagina1.xyz/\">Enlace 1</a>\n",
    "            <a href=\"https://pagina2.xyz/\">Enlace 2</a>\n",
    "        </div>\n",
    "        <div class=\"right\">\n",
    "            <div class=\"links\">\n",
    "                <a href=\"https://pagina3.xyz/\">Enlace 3</a>\n",
    "                <a href=\"https://pagina4.xyz/\">Enlace 4</a>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "    <div id=\"footer\">\n",
    "        <!-- El footer -->\n",
    "        <p>Este párrafo está en el footer</p>\n",
    "        <div class=\"links footer-links\">\n",
    "            <a href=\"https://pagina5.xyz/\">Enlace 5</a>\n",
    "        </div>\n",
    "    </div>\n",
    "</div>\n",
    "</body>\n",
    "</html>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"index.html\", \"w\") as f:\n",
    "    f.write(html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists to use in dataframe\n",
    "title = []\n",
    "id_div = []\n",
    "tag_name = []\n",
    "value = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "title ['Página de prueba']\nid ['main']\ntag ['p']\nvalue ['Este es el primer párrafo']\n"
    }
   ],
   "source": [
    "#1#finding title and adding to list to use in df.\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "title.append(title_content_str)\n",
    "print(\"title\", title)\n",
    "\n",
    "#1# getting the id name: Main and adding to list\n",
    "result = []\n",
    "for tag in soup.findAll(True,{'id':True}) :\n",
    "    result.append(tag['id'])\n",
    "\n",
    "id_div.append(result[0])\n",
    "print(\"id\", id_div)\n",
    "\n",
    "#1# find the tag: p and select the name of the tag and adding to list for df\n",
    "soup.find(\"p\").name\n",
    "tag_name.append(soup.find(\"p\").name)\n",
    "\n",
    "print(\"tag\", tag_name)\n",
    "\n",
    "#1# find the content in tag p and adding to list for df\n",
    "soup.find(\"p\").contents\n",
    "value.append(soup.find(\"p\").contents[0])\n",
    "print(\"value\", value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Página de prueba', 'Página de prueba']\n['main', nan]\n['p', 'a']\n['Este es el primer párrafo', 'https://pagina1.xyz/']\n"
    }
   ],
   "source": [
    "#2#finding title and adding to list to use in df.\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "title.append(title_content_str)\n",
    "print(title)\n",
    "\n",
    "#2# checked where https://pagina1.xyz/ is stored and no id so added none to list for\n",
    "result = soup.find(\"div\", {\"class\":\"links\"})\n",
    "id_div.append(np.nan)\n",
    "print(id_div)\n",
    "\n",
    "#2# find tag name and added to list for df\n",
    "soup.find('a').name\n",
    "tag_name.append(soup.find('a').name)\n",
    "print(tag_name)\n",
    "\n",
    "#2# find content of link and added to value list for df\n",
    "link1 = soup.find(\"a\")\n",
    "content = link1.get('href')\n",
    "value.append(content)\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Página de prueba', 'Página de prueba', 'Página de prueba']\n['main', nan, nan]\n['p', 'a', 'a']\n['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/']\n"
    }
   ],
   "source": [
    "#3# title added to lsit\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "title.append(title_content_str)\n",
    "print(title)\n",
    "\n",
    "#3# no id in div, added none to list for df\n",
    "result = soup.find(\"div\", {\"class\":\"links\"})\n",
    "id_div.append(np.nan)\n",
    "print(id_div)\n",
    "\n",
    "#3# find tag name a and added to list\n",
    "soup.find('a').name\n",
    "tag_name.append(soup.find('a').name)\n",
    "print(tag_name)\n",
    "\n",
    "#3# finding the link and adding to value\n",
    "links = soup.findAll(\"a\")\n",
    "for pos, link in enumerate(links):\n",
    "    if pos == 3:\n",
    "        href = link.get('href')\n",
    "        value.append(href)\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n['main', nan, nan, nan]\n['p', 'a', 'a', 'a']\n['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/']\n"
    }
   ],
   "source": [
    "#4# title added to lsit\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "title.append(title_content_str)\n",
    "print(title)\n",
    "\n",
    "#4# no id in div, added none to list for df\n",
    "result = soup.find(\"div\", {\"class\":\"links\"})\n",
    "id_div.append(np.nan)\n",
    "print(id_div)\n",
    "\n",
    "#4# find tag name a and added to list\n",
    "soup.find('a').name\n",
    "tag_name.append(soup.find('a').name)\n",
    "print(tag_name)\n",
    "\n",
    "#4# finding the link and adding to value\n",
    "links = soup.findAll(\"a\")\n",
    "for pos, link in enumerate(links):\n",
    "    if pos == 4:\n",
    "        href = link.get('href')\n",
    "        value.append(href)\n",
    "\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n['main', nan, nan, nan, 'footer']\n['p', 'a', 'a', 'a', 'div']\n['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/', 'links footer-links']\n"
    }
   ],
   "source": [
    "#5# title added to lsit\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "title.append(title_content_str)\n",
    "print(title)\n",
    "\n",
    "#5# id of div is footer. finding id name and adding to list. \n",
    "result = []\n",
    "for tag in soup.findAll(True,{'id':True}) :\n",
    "    result.append(tag['id'])\n",
    "\n",
    "id_div.append(result[2])\n",
    "print(id_div)\n",
    "\n",
    "#5# find tag name: div a and added to list\n",
    "soup.find(\"div\").name\n",
    "tag_name.append(soup.find(\"div\").name)\n",
    "print(tag_name)\n",
    "\n",
    "#5# finding the value links footer-links and adding to value\n",
    "all_div =  soup.find(\"div\", id=\"footer\")\n",
    "all_div_1 = all_div.find(\"div\")\n",
    "result = all_div_1.get_attribute_list('class')\n",
    "final_result = [str(result[0])+ \" \" + str(result[1])]\n",
    "value.append(final_result[0])\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n['main', nan, nan, nan, 'footer', 'footer']\n['p', 'a', 'a', 'a', 'div', 'p']\n['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/', 'links footer-links', 'Este párrafo está en el footer']\n"
    }
   ],
   "source": [
    "#6# title added to lsit\n",
    "title_content = soup.find('title')\n",
    "title_content_str = title_content.string\n",
    "title.append(title_content_str)\n",
    "print(title)\n",
    "\n",
    "#6# id of div is footer. finding id name and adding to list. \n",
    "result = []\n",
    "for tag in soup.findAll(True,{'id':True}) :\n",
    "    result.append(tag['id'])\n",
    "\n",
    "id_div.append(result[2])\n",
    "print(id_div)\n",
    "\n",
    "#6# find the tag: p and select the name of the tag and adding to list for df\n",
    "soup.find(\"p\").name\n",
    "tag_name.append(soup.find(\"p\").name)\n",
    "\n",
    "print(tag_name)\n",
    "\n",
    "#5# finding the value 'Este párrafo está en el footer' and adding to value\n",
    "content_p = soup.findAll(\"p\")\n",
    "for pos, elem in enumerate(content_p):\n",
    "    if pos == 2:\n",
    "        content_value = elem.contents\n",
    "        value.append(content_value[0])\n",
    "        \n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba', 'Página de prueba']\n['main', nan, nan, nan, 'footer', 'footer']\n['p', 'a', 'a', 'a', 'div', 'p']\n['Este es el primer párrafo', 'https://pagina1.xyz/', 'https://pagina4.xyz/', 'https://pagina5.xyz/', 'links footer-links', 'Este párrafo está en el footer']\n"
    }
   ],
   "source": [
    "#printing final lists:\n",
    "\n",
    "print(title)\n",
    "print(id_div)\n",
    "print(tag_name)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating dataframe \n",
    "df = pd.DataFrame({\"Title\": title, \"Id\": id_div, \"Tag\": tag_name, \"Value\": value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              Title      Id  Tag                           Value\n0  Página de prueba    main    p       Este es el primer párrafo\n1  Página de prueba     NaN    a            https://pagina1.xyz/\n2  Página de prueba     NaN    a            https://pagina4.xyz/\n3  Página de prueba     NaN    a            https://pagina5.xyz/\n4  Página de prueba  footer  div              links footer-links\n5  Página de prueba  footer    p  Este párrafo está en el footer",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Title</th>\n      <th>Id</th>\n      <th>Tag</th>\n      <th>Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Página de prueba</td>\n      <td>main</td>\n      <td>p</td>\n      <td>Este es el primer párrafo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Página de prueba</td>\n      <td>NaN</td>\n      <td>a</td>\n      <td>https://pagina1.xyz/</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Página de prueba</td>\n      <td>NaN</td>\n      <td>a</td>\n      <td>https://pagina4.xyz/</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Página de prueba</td>\n      <td>NaN</td>\n      <td>a</td>\n      <td>https://pagina5.xyz/</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Página de prueba</td>\n      <td>footer</td>\n      <td>div</td>\n      <td>links footer-links</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Página de prueba</td>\n      <td>footer</td>\n      <td>p</td>\n      <td>Este párrafo está en el footer</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-37-d93176edb17c>, line 1)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-37-d93176edb17c>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    1. From Amazon\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "1. From Amazon\n",
    "Using beautiful soap and/or regex\n",
    "\n",
    "Save in a dataframe the next information using web scraping. Using product pages from Amazon, do the following:\n",
    "\n",
    "Get the product name from the web and save it in a column called \"item_name\"\n",
    "Get the price from the web and save it in a column called \"item_price\"\n",
    "While you are doing the exercise, document the steps you are doing. Try to do the program for generic pages. If you cannot do it generic, explain the reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-307c183594cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mlista\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id=.*\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mlista\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "#x = [re.search('\"\\w+\"', e).group()[1:-1] for e in re.findall(\"id=.*\",str(soup.contents[0]))]\n",
    "\n",
    "#x\n",
    "\n",
    "\n",
    "lista = re.findall(\"id=.*\",str(soup.contents[0]))\n",
    "lista\n",
    "\n",
    "for e in lista:\n",
    "    print(re.search('\"\\w+\"', e).group()[1:-1])"
   ]
  }
 ]
}